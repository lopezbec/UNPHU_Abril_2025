{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><b>© Content is made available under the CC-BY-NC-ND 4.0 license.\n",
        "\n",
        "---\n",
        "\n",
        "Christian Lopez, lopezbec@lafayette.edu<center>"
      ],
      "metadata": {
        "id": "pXP789oCH06g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Este notebook demuestra la generación interactiva de texto utilizando el modelo GPT‑2 Large. Podrás introducir un prompt y luego generar texto de forma interactiva, token por token, o especificar el número de tokens a generar mediante un formulario sencillo. El notebook muestra detalles como la tokenización y las principales predicciones de tokens antes de la generación.\n",
        "\n",
        "# Acerca de GPT‑2 Large:\n",
        "## GPT‑2 Large es uno de los modelos de lenguaje de código abierto desarrollado por OpenAI, con 774 millones de parámetros."
      ],
      "metadata": {
        "id": "g5V7-YkXNMVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1) Instalando unas librerías de Python y descargando el modelo GPT-2\n",
        "# Install the transformers library (if not already installed)\n",
        "!pip install transformers\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Use GPT-2 Large as our model for English text generation\n",
        "model_name = \"gpt2-large\"  # GPT-2 Large has ~774M parameters\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Check if GPU is available and move the model to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GRmw2pgoHsxD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2) Aquí puedes escribir tu prompt , para ver como se convierte en una lista de números\n",
        "# Get a prompt from the user\n",
        "prompt ='What is the capital of Dominican Repulbic?' # @param {type:\"string\", placeholder:\"Enter a prompt\"}\n",
        "\n",
        "# Tokenize the prompt and print tokens and IDs\n",
        "tokenized_prompt = tokenizer.tokenize(prompt)\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "print(\"\\n--- Tokenization Details ---\")\n",
        "print(\"Tokenized prompt (tokens):\", tokenized_prompt)\n",
        "print(\"Tokenized prompt (token IDs):\", input_ids.tolist()[0])\n",
        "\n",
        "# Set temperature for randomness (lower value = more deterministic)\n",
        "temperature = 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "mruIJYc6H-8N",
        "outputId": "9287f484-a1f9-49e9-8208-a654e7a500c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tokenization Details ---\n",
            "Tokenized prompt (tokens): ['What', 'Ġis', 'Ġthe', 'Ġcapital', 'Ġof', 'ĠDominican', 'ĠRep', 'ul', 'b', 'ic', '?']\n",
            "Tokenized prompt (token IDs): [2061, 318, 262, 3139, 286, 32022, 1432, 377, 65, 291, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "orfP46_CjCAO",
        "outputId": "37a28690-e76e-4e0d-8ee3-f35a2da1a8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top 5 Tokens for the Next Prediction ---\n",
            "Token: '', Probability: 1.0000\n",
            "Token: 'It', Probability: 0.0000\n",
            "Token: '', Probability: 0.0000\n",
            "Token: 'The', Probability: 0.0000\n",
            "Token: 'Is', Probability: 0.0000\n",
            "\n",
            "--- Interactive Generation Process ---\n",
            "\n",
            "Step 1: Predicted token: '' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "----------------------------------------\n",
            "Step 2: Predicted token: '' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Step 3: Predicted token: 'The' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The\n",
            "----------------------------------------\n",
            "Step 4: Predicted token: 'capital' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital\n",
            "----------------------------------------\n",
            "Step 5: Predicted token: 'of' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of\n",
            "----------------------------------------\n",
            "Step 6: Predicted token: 'Dominican' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican\n",
            "----------------------------------------\n",
            "Step 7: Predicted token: 'Rep' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Rep\n",
            "----------------------------------------\n",
            "Step 8: Predicted token: 'ul' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repul\n",
            "----------------------------------------\n",
            "Step 9: Predicted token: 'b' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulb\n",
            "----------------------------------------\n",
            "Step 10: Predicted token: 'ic' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic\n",
            "----------------------------------------\n",
            "Step 11: Predicted token: 'is' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic is\n",
            "----------------------------------------\n",
            "Step 12: Predicted token: 'Sant' with probability: 0.9967\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic is Sant\n",
            "----------------------------------------\n",
            "Step 13: Predicted token: 'o' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic is Santo\n",
            "----------------------------------------\n",
            "Step 14: Predicted token: 'D' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic is Santo D\n",
            "----------------------------------------\n",
            "Step 15: Predicted token: 'oming' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic is Santo Doming\n",
            "----------------------------------------\n",
            "Step 16: Predicted token: 'o' with probability: 1.0000\n",
            "Generate next token? (y/n): y\n",
            "\n",
            "Updated text:\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic is Santo Domingo\n",
            "----------------------------------------\n",
            "Step 17: Predicted token: '.' with probability: 0.9913\n",
            "Generate next token? (y/n): n\n",
            "\n",
            "--- Full Generated Response ---\n",
            "\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic is Santo Domingo.\n"
          ]
        }
      ],
      "source": [
        "# @title 3) Esto es para generar la predicaron de las siguientes palaba, una a la vez\n",
        "\n",
        "\n",
        "\n",
        "# Compute logits for the current prompt\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Adjust logits with temperature for randomness and compute softmax probabilities\n",
        "next_token_logits = logits[:, -1, :] / temperature\n",
        "probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "# Get the top 5 tokens with the highest probabilities\n",
        "top5 = torch.topk(probs, 5)\n",
        "top5_ids = top5.indices[0]\n",
        "top5_probs = top5.values[0]\n",
        "top5_tokens = [tokenizer.decode(token_id) for token_id in top5_ids]\n",
        "\n",
        "print(\"\\n--- Top 5 Tokens for the Next Prediction ---\")\n",
        "for token, prob in zip(top5_tokens, top5_probs):\n",
        "    print(f\"Token: '{token.strip()}', Probability: {prob.item():.4f}\")\n",
        "\n",
        "# Initialize generated text with the prompt\n",
        "generated_text = prompt\n",
        "\n",
        "# Define the maximum number of tokens to generate in the interactive loop\n",
        "max_tokens = 50  # Adjust as needed\n",
        "\n",
        "print(\"\\n--- Interactive Generation Process ---\\n\")\n",
        "\n",
        "# Interactive loop: generate tokens one at a time based on user confirmation\n",
        "for i in range(max_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Adjust logits with temperature for randomness\n",
        "    next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    # Sample the next token from the probability distribution\n",
        "    next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "    next_token_prob = probs[0, next_token_id.item()]  # probability of the chosen token\n",
        "\n",
        "    # Decode the predicted token to a string\n",
        "    next_token = tokenizer.decode(next_token_id.squeeze())\n",
        "\n",
        "    # Append the predicted token to the input_ids for the next iteration\n",
        "    input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "    # Append the predicted token to the generated text\n",
        "    generated_text += next_token\n",
        "\n",
        "    # Print the predicted token and its probability\n",
        "    print(f\"Step {i+1}: Predicted token: '{next_token.strip()}' with probability: {next_token_prob.item():.4f}\")\n",
        "\n",
        "    # Ask the user if they want to continue generating the next token\n",
        "    user_input = input(\"Generate next token? (y/n): \")\n",
        "    if user_input.lower() not in [\"y\", \"yes\"]:\n",
        "        break\n",
        "\n",
        "    # Print the updated prompt/text after the user confirms\n",
        "    print(\"\\nUpdated text:\")\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n--- Full Generated Response ---\\n\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Aquí todo de una vez\n",
        "\n",
        "Trata de cambiar el valor de parametro `temperature` para ser mas \"creativo\""
      ],
      "metadata": {
        "id": "NsUhOXazOqlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "# Get the initial prompt from the user\n",
        "prompt ='What is the capital of Dominican Repulbic?' # @param {type:\"string\", placeholder:\"Enter a prompt\"}\n",
        "\n",
        "# Tokenize the prompt and encode to tensor\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "generated_text = prompt\n",
        "\n",
        "# Ask the user for the number of new tokens to generate\n",
        "num_tokens = 25 # @param {type:\"number\"}\n",
        "\n",
        "# Set temperature for randomness (lower value means more deterministic output)\n",
        "temperature = 0.1 # @param {type:\"slider\", min:0.1, max:0.9, step:0.1}\n",
        "\n",
        "print(\"\\n--- Generating Text ---\\n\")\n",
        "for i in range(num_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Adjust logits with temperature for randomness and compute probabilities\n",
        "    next_token_logits = logits[:, -1, :] / temperature\n",
        "    probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    # Sample the next token from the probability distribution\n",
        "    next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "    next_token = tokenizer.decode(next_token_id.squeeze())\n",
        "\n",
        "    # Append the predicted token to input_ids and generated text\n",
        "    input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "    generated_text += next_token\n",
        "\n",
        "    # Optionally, print each step\n",
        "    print(f\"Step {i+1}: Generated token: '{next_token.strip()}'\")\n",
        "\n",
        "print(\"\\n--- Final Generated Text ---\\n\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "7WtGjK01JlCr",
        "outputId": "ed1a3913-7f06-48eb-afc5-08f03d61bbbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Text ---\n",
            "\n",
            "Step 1: Generated token: ''\n",
            "Step 2: Generated token: ''\n",
            "Step 3: Generated token: 'The'\n",
            "Step 4: Generated token: 'capital'\n",
            "Step 5: Generated token: 'of'\n",
            "Step 6: Generated token: 'Dominican'\n",
            "Step 7: Generated token: 'Rep'\n",
            "Step 8: Generated token: 'ul'\n",
            "Step 9: Generated token: 'b'\n",
            "Step 10: Generated token: 'ic'\n",
            "Step 11: Generated token: 'is'\n",
            "Step 12: Generated token: 'Sant'\n",
            "Step 13: Generated token: 'o'\n",
            "Step 14: Generated token: 'D'\n",
            "Step 15: Generated token: 'oming'\n",
            "Step 16: Generated token: 'o'\n",
            "Step 17: Generated token: '.'\n",
            "Step 18: Generated token: ''\n",
            "Step 19: Generated token: ''\n",
            "Step 20: Generated token: 'What'\n",
            "Step 21: Generated token: 'is'\n",
            "Step 22: Generated token: 'the'\n",
            "Step 23: Generated token: 'capital'\n",
            "Step 24: Generated token: 'of'\n",
            "Step 25: Generated token: 'Dominican'\n",
            "\n",
            "--- Final Generated Text ---\n",
            "\n",
            "What is the capital of Dominican Repulbic?\n",
            "\n",
            "The capital of Dominican Repulbic is Santo Domingo.\n",
            "\n",
            "What is the capital of Dominican\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursos Adicionales\n",
        "\n",
        "Si estás interesado en aprender más sobre los fundamentos de los modelos de lenguaje a gran escala (LLMs) y cómo funcionan, aquí tienes algunos recursos útiles:\n",
        "\n",
        "### Sitios Web\n",
        "- **[Documentación de Hugging Face Transformers](https://huggingface.co/transformers/):**  \n",
        "  Una guía completa sobre cómo utilizar modelos basados en transformadores, que incluye tutoriales y ejemplos de código.\n",
        "- **[Blog de OpenAI](https://openai.com/blog/):**  \n",
        "  Lee sobre las últimas investigaciones, aplicaciones y debates acerca de modelos de lenguaje avanzados.\n",
        "- **[El Transformer Ilustrado](http://jalammar.github.io/illustrated-transformer/):**  \n",
        "  Una explicación visual e intuitiva de las arquitecturas de transformadores y de cómo impulsan los modelos de lenguaje modernos.\n",
        "- **[Stanford CS224n: Procesamiento del Lenguaje Natural con Deep Learning](https://web.stanford.edu/class/cs224n/):**  \n",
        "  Explora apuntes, diapositivas y grabaciones en video sobre PNL y deep learning.\n",
        "\n",
        "### Canales y Videos en YouTube\n",
        "- **Hugging Face – Transformers en Acción:**  \n",
        "  Busca tutoriales prácticos y demostraciones sobre el uso de transformadores en el [Canal de YouTube de Hugging Face](https://www.youtube.com/c/HuggingFace).\n",
        "- **Yannic Kilcher:**  \n",
        "  Análisis en profundidad de artículos de investigación y modelos de IA, incluyendo explicaciones de GPT-2, GPT-3 y otros LLMs. Visita su [canal de YouTube](https://www.youtube.com/c/YannicKilcher).\n",
        "- **Two Minute Papers:**  \n",
        "  Resúmenes cortos y accesibles de investigaciones recientes en IA, que incluyen avances en modelos de lenguaje. Visita el [Canal de YouTube de Two Minute Papers](https://www.youtube.com/user/keeroyz).\n",
        "- **DeepLearning.AI:**  \n",
        "  Contenido educativo y discusiones sobre las tendencias actuales en deep learning e IA. Mira videos en el [Canal de YouTube de DeepLearning.AI](https://www.youtube.com/c/Deeplearningai).\n",
        "\n",
        "Estos recursos te ayudarán a profundizar en tu comprensión de los LLMs y a explorar más sobre el emocionante campo del procesamiento del lenguaje natural.\n"
      ],
      "metadata": {
        "id": "7EUGtx-3RWFi"
      }
    }
  ]
}